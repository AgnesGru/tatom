{
 "metadata": {
  "name": "Working with text (Introduction to IPython)"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Working with text\n=================\n\nThis is a shortened version of the tutorial \"Working with text\" in the form of an interactive IPython notebook. This document is \"live\"; any code example can be edited and executed in the browser. To see this in action, change some part of the code in the *cell* below and then click on the play button above."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "list_of_strings = ['working', 'with', 'text']\nfor s in list_of_strings:\n    print(s)",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "working\nwithd\ntext\n"
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Creating a document-term matrix\n-------------------------------\n\nWord frequencies and document-term matrices are typical units of\nanalysis when working with text collections. It may come as a surprise\nthat reducing a book to a list of word frequencies retains any useful\ninformation, but practice has shown this to be the case. Treating texts\nas a list of word frequencies (a vector) also makes available a range of\nmathematical tools developed for [studying and manipulating\nvectors](http://en.wikipedia.org/wiki/Euclidean_vector#History).\n\n> **note**\n>\n> Turning texts into unordered lists (or \"bags\") of words is easy in\n> :   Python. [Python Programming for the\n>     Humanities](http://fbkarsdorp.github.io/python-course/) includes a\n>     chapter entitled [Text\n>     Processing](http://nbviewer.ipython.org/urls/raw.github.com/fbkarsdorp/python-course/master/Chapter%203%20-%20Text%20Preprocessing.ipynb)\n>     that describes the steps in detail.\n>\nThis document assumes some prior exposure to text analysis so we will\ngather word frequencies (or term frequencies) derived from the lists of\nwords appearing in texts into a document-term matrix using the\n[CountVectorizer](http://scikit-learn.sourceforge.net/dev/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)\nclass from the [scikit-learn](http://scikit-learn.sourceforge.net/)\npackage. (For those familiar with R and the\n[tm](http://cran.r-project.org/web/packages/tm/) package, this function\nperforms the same operation as `DocumentTermMatrix` and takes\nrecognizably similar arguments.)\n\nThis document assumes some prior exposure to text analysis so we will\ngather word frequencies (or term frequencies) derived from the lists of\nwords appearing in texts into a document-term matrix using the\n[CountVectorizer](http://scikit-learn.sourceforge.net/dev/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)\nclass from the [scikit-learn](http://scikit-learn.sourceforge.net/)\npackage. (For those familiar with R and the\n[tm](http://cran.r-project.org/web/packages/tm/) package, this function\nperforms the same operation as `DocumentTermMatrix` and takes\nrecognizably similar arguments.)\n\nFirst we need to import the functions and classes we intend to use,\nalong with our customary abbreviation for functions in the `numpy`\npackage."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "import numpy as np  # a conventional alias\nfrom sklearn.feature_extraction.text import CountVectorizer",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Now we use the\n[CountVectorizer](http://scikit-learn.sourceforge.net/dev/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)\nclass to create a document-term matrix. `CountVectorizer` is highly\ncustomizable. For example, a list of \"stop words\" can be specified with\nthe stop\\_words parameter. Other important parameters include:\n\n-   `lowercase` (default `True`) convert all text to lowercase before\n    tokenizing\n-   `min_df` (default `1`) remove from the vocabulary terms that occur\n    in fewer than `min_df` documents\u2013with a large corpus this may be set\n    to `5` to eliminate rare words\n-   `vocabulary` ignore words that do not appear in the list (or\n    iterable) assigned to parameter `vocabulary`\n-   `strip_accents` remove accents\n-   `token_pattern` (default `u'(?u)\\b\\w\\w+\\b'`) regular expression\n    identifying tokens\u2013by default words that consist of a single\n    character (e.g., 'a', '2') are ignored, setting `token_pattern` to\n    `'(?u)\\b\\w+\\b'` will include these tokens\n-   `tokenizer` (default unused) use a custom function for tokenizing\n"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "filenames = ['data/austen-bront\u00eb/Austen_Emma.txt',           \n             'data/austen-bront\u00eb/Austen_Pride.txt',          \n             'data/austen-bront\u00eb/Austen_Sense.txt',          \n             'data/austen-bront\u00eb/CBronte_Jane.txt',          \n             'data/austen-bront\u00eb/CBronte_Professor.txt',     \n             'data/austen-bront\u00eb/CBronte_Villette.txt']      \n                                                             \nvectorizer = CountVectorizer(input='filename')               \ndtm = vectorizer.fit_transform(filenames)  # a sparse matrix \nvocab_list = vectorizer.get_feature_names()                  \n",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Now we have a document-term matrix and a vocabulary list. Before we can\nquery the matrix and find out, for example, how many times the word\n'house' occurs in *Emma* (the first text in `filenames`), we need to\nconvert this matrix from its current format, a [sparse\nmatrix](http://docs.scipy.org/doc/scipy/reference/sparse.html), into a\nnormal NumPy array. We will also convert `vocab`, a list of vocabulary,\nto an array of strings, as an array supports a greater variety of\noperations.\n"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "# for reference, note the current class of `dtm`  \ntype(dtm)                                         \ndtm = dtm.toarray()  # convert to a regular array \nvocab = np.array(vocab_list)",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "> **note**\n>\n> A sparse matrix is used to store matrices that contain a significant\n> :   number of entries that are zero. Typically, a sparse matrix only\n>     records non-zero entries. To understand why this matters so much\n>     that `CountVectorizer` returns a sparse matrix by default,\n>     consider a 4000 by 50000 matrix that is 60% zeros. In Python an\n>     integer takes up 4 bytes, so using a sparse matrix saves almost\n>     500M, which is a significant amount of computer memory. (Remember\n>     that arrays are usually stored in memory, not on disk).\n>\nQuerying the document-term matrix and the vocabulary is straightforward.\nFor example, here are two ways of finding how many times the word\n'house' occurs in the first text, *Emma*:\n"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "# the first file, indexed by 0 in Python, is *Emma*                 \nfilenames[0] == 'data/austen-bront\u00eb/Austen_Emma.txt'                \n                                                                    \n# use the standard Python list method index(...)                    \nhouse_idx = vocab_list.index('house')                               \ndtm[0, house_idx]                                                   \n                                                                    \n# alternatively, use NumPy indexing                                 \n# in R this would be essentially the same, dtm[1, vocab == 'house'] \ndtm[0, vocab == 'house']                                         \n",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 11,
       "text": "array([95])"
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "# verify that this is the result we anticipated\nvocab[house_idx]",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 13,
       "text": "'house'"
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Sandbox\n=======\nFeel free to experiment with the document-term matrix `dtm` in the code cells below."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "print(dtm.shape)\nfor fn in filenames:\n    print(fn)",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "(6, 12894)\ndata/austen-bront\u00eb/Austen_Emma.txt\ndata/austen-bront\u00eb/Austen_Pride.txt\ndata/austen-bront\u00eb/Austen_Sense.txt\ndata/austen-bront\u00eb/CBronte_Jane.txt\ndata/austen-bront\u00eb/CBronte_Professor.txt\ndata/austen-bront\u00eb/CBronte_Villette.txt\n"
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "print(len(vocab))\nvocab[500:550]  # look at some of the vocabulary",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "12894\n"
      },
      {
       "output_type": "pyout",
       "prompt_number": 20,
       "text": "array(['amendment', 'amends', 'amenity', 'ami', 'amiability', 'amiable',\n       'amicable', 'amid', 'amidst', 'amie', 'amiss', 'amity', 'among',\n       'amongst', 'amount', 'amounted', 'amounting', 'amour',\n       'amphitheatre', 'ample', 'amplifications', 'amplitude', 'amply',\n       'amuse', 'amused', 'amusement', 'amusements', 'amuses', 'amusing',\n       'an', 'analogy', 'analysis', 'ancestors', 'anchor', 'ancient',\n       'and', 'anecdote', 'anecdotes', 'anew', 'ange', 'angel', 'angelic',\n       'angels', 'anger', 'anges', 'anglais', 'anglaise', 'anglaises',\n       'angle', 'angles'], \n      dtype='<U17')"
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "",
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}